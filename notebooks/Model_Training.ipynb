{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3c4383",
   "metadata": {},
   "source": [
    "### Caveats or Process Did\n",
    "\n",
    "By combining the training and test sets before feature engineering, we make sure that these features are computed consistently and accurately, even across the boundary between the last training date and the first test date. This prevents missing or incorrect values for the first test rows, maintains continuity for rolling windows and momentum indicators, and avoids artificial discontinuities in computed statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2a73d",
   "metadata": {},
   "source": [
    "#### Problems that arose (1)[Negative Values]:\n",
    "\n",
    "Approach(1)\n",
    "- Getting me non-sense cumalative and SHARPE ratio, negative values\n",
    "- Probable causes: since I am apply the transform method the to the test set alone, it causes my lag, rolling and EMA features to be most incorrect for the first few rows of the test set, which might throw of my sharpe-ratio\n",
    "- Soln: is to combine last few rows of trainging set with test set so that the first few rows of the test set would have valid values for the lag-features\n",
    "\n",
    "- No need to worry about data leakage and lookforward bias, since I am only using my previous few computes features to compute the transformations in my test-sets, thus, I am only using my previous values.\n",
    "\n",
    "-Feedback: Didn't work\n",
    "\n",
    "Approach (2):\n",
    "- Since market_excess_returns, represent a fraction probably need to scale them\n",
    "- Box-Cox Transformations.\n",
    "\n",
    "**REMARK**: Currently only trained on the training set, with held out test set. Need to integrate the the test set by combining the dataset, then splitting the dataset using TimeSeriesSplit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4927c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856f1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesEngineering:\n",
    "    def __init__(self,\n",
    "                 base_features,\n",
    "                 lag_windows=[1,5,10],\n",
    "                 rolling_windows=[5,10,20],\n",
    "                 ema_windows=[10,30,60],\n",
    "                 eps=1e-8,\n",
    "                 sparse_cols=None,\n",
    "                 clip_quantiles=(0.01,0.99),\n",
    "                 scale_features=True):\n",
    "        self.base_features = base_features\n",
    "        self.lag_windows = lag_windows\n",
    "        self.rolling_windows = rolling_windows\n",
    "        self.ema_windows = ema_windows\n",
    "        self.eps = eps\n",
    "        self.sparse_cols = sparse_cols or []\n",
    "        self.clip_quantiles = clip_quantiles\n",
    "        self.scale_features_flag = scale_features\n",
    "        self.scaler = None\n",
    "\n",
    "        # Store statistics computed on training data\n",
    "        self.clip_bounds = {}\n",
    "        self.boxcox_lambdas = {}\n",
    "        self.boxcox_shifts = {}\n",
    "\n",
    "    def drop_sparse(self, df):\n",
    "        df = df.drop(columns=[c for c in self.sparse_cols if c in df.columns], errors='ignore')\n",
    "        return df\n",
    "\n",
    "    def forward_fill(self, df):\n",
    "        df = df.sort_values(by='date_id').copy()\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "        df.fillna(0, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def clip_outliers(self, df, fit=True):\n",
    "        \"\"\"Clip outliers using bounds computed on training data\"\"\"\n",
    "        for col in self.base_features:\n",
    "            if col in df.columns:\n",
    "                if fit:\n",
    "                    # Compute and store bounds on training data\n",
    "                    lower = df[col].quantile(self.clip_quantiles[0])\n",
    "                    upper = df[col].quantile(self.clip_quantiles[1])\n",
    "                    self.clip_bounds[col] = (lower, upper)\n",
    "\n",
    "                # Apply stored bounds\n",
    "                if col in self.clip_bounds:\n",
    "                    lower, upper = self.clip_bounds[col]\n",
    "                    df[col] = df[col].clip(lower, upper)\n",
    "        return df\n",
    "\n",
    "    def apply_boxcox(self, df, fit=True):\n",
    "        \"\"\"Apply Box-Cox transformation using parameters from training data\"\"\"\n",
    "        EPS = self.eps\n",
    "        for col in self.base_features:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            if fit:\n",
    "                # Compute shift and lambda on training data\n",
    "                min_val = df[col].min()\n",
    "                shift = -min_val + EPS if min_val <= 0 else 0\n",
    "                shifted = df[col] + shift\n",
    "                shifted[shifted <= 0] = EPS\n",
    "\n",
    "                try:\n",
    "                    transformed, lmbda = boxcox(shifted)\n",
    "                    df[col] = transformed\n",
    "                    self.boxcox_lambdas[col] = lmbda\n",
    "                    self.boxcox_shifts[col] = shift\n",
    "                except ValueError:\n",
    "                    # If Box-Cox fails, store None to skip this column\n",
    "                    self.boxcox_lambdas[col] = None\n",
    "                    continue\n",
    "            else:\n",
    "                # Apply stored transformation\n",
    "                if col in self.boxcox_lambdas and self.boxcox_lambdas[col] is not None:\n",
    "                    lmbda = self.boxcox_lambdas[col]\n",
    "                    shift = self.boxcox_shifts[col]\n",
    "                    shifted = df[col] + shift\n",
    "                    shifted[shifted <= 0] = EPS\n",
    "\n",
    "                    # Apply Box-Cox with stored lambda\n",
    "                    if lmbda == 0:\n",
    "                        df[col] = np.log(shifted)\n",
    "                    else:\n",
    "                        df[col] = (shifted**lmbda - 1) / lmbda\n",
    "        return df\n",
    "\n",
    "    def scale_features(self, df, fit=True):\n",
    "        \"\"\"Scale features using StandardScaler\"\"\"\n",
    "        if not self.scale_features_flag:\n",
    "            return df\n",
    "\n",
    "        if fit:\n",
    "            self.scaler = StandardScaler()\n",
    "            df[self.base_features] = self.scaler.fit_transform(df[self.base_features])\n",
    "        else:\n",
    "            if self.scaler is None:\n",
    "                raise ValueError(\"Scaler not fitted. Call transform with fit=True first.\")\n",
    "            df[self.base_features] = self.scaler.transform(df[self.base_features])\n",
    "        return df\n",
    "\n",
    "    def add_lag(self, df):\n",
    "        \"\"\"Add lag features\"\"\"\n",
    "        for col in self.base_features:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            for lag in self.lag_windows:\n",
    "                df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
    "        return df\n",
    "\n",
    "    def add_rolling(self, df):\n",
    "        \"\"\"Add rolling window features\"\"\"\n",
    "        for col in self.base_features:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            for w in self.rolling_windows:\n",
    "                # Use shift(1) to avoid look-ahead bias\n",
    "                shifted = df[col].shift(1)\n",
    "                df[f\"{col}_roll_mean_{w}\"] = shifted.rolling(w).mean()\n",
    "                df[f\"{col}_roll_std_{w}\"] = shifted.rolling(w).std()\n",
    "                df[f\"{col}_roll_min_{w}\"] = shifted.rolling(w).min()\n",
    "                df[f\"{col}_roll_max_{w}\"] = shifted.rolling(w).max()\n",
    "        return df\n",
    "\n",
    "    def add_ema(self, df):\n",
    "        \"\"\"Add exponential moving average features\"\"\"\n",
    "        for col in self.base_features:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            for w in self.ema_windows:\n",
    "                df[f\"{col}_ema_{w}\"] = df[col].shift(1).ewm(span=w, adjust=False).mean()\n",
    "        return df\n",
    "\n",
    "    def transform(self, df, fit=True):\n",
    "        \"\"\"\n",
    "        Transform the dataframe with feature engineering.\n",
    "\n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            fit: If True, fit transformations on this data (use for training).\n",
    "                 If False, apply previously fitted transformations (use for validation/test).\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df = self.drop_sparse(df)\n",
    "        df = self.forward_fill(df)\n",
    "        df = self.clip_outliers(df, fit=fit)\n",
    "        df = self.apply_boxcox(df, fit=fit)\n",
    "        df = self.scale_features(df, fit=fit)\n",
    "        df = self.add_lag(df)\n",
    "        df = self.add_rolling(df)\n",
    "        df = self.add_ema(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81288028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesModel:\n",
    "    def __init__(self,\n",
    "                 target_col='market_forward_excess_returns',\n",
    "                 date_col='date_id',\n",
    "                 n_splits=5,\n",
    "                 params=None,\n",
    "                 n_estimators=1000,\n",
    "                 early_stopping_rounds=50,\n",
    "                 verbose=True):\n",
    "        self.target_col = target_col\n",
    "        self.date_col = date_col\n",
    "        self.n_splits = n_splits\n",
    "        self.params = params if params else {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 5,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'n_jobs': -1,\n",
    "            'tree_method': 'hist',\n",
    "            'random_state': 42\n",
    "        }\n",
    "        self.n_estimators = n_estimators\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.verbose = verbose\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, df, features):\n",
    "        df = df.sort_values(by=self.date_col).copy()\n",
    "        X = df[features].values\n",
    "        y = df[self.target_col].values\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=self.n_splits)\n",
    "        fold = 1\n",
    "\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            model = xgb.train(\n",
    "                params=self.params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=self.n_estimators,\n",
    "                evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                early_stopping_rounds=self.early_stopping_rounds,\n",
    "                verbose_eval=self.verbose\n",
    "            )\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Fold {fold} finished. Best Iteration: {model.best_iteration}\")\n",
    "\n",
    "            self.models.append(model)\n",
    "            fold += 1\n",
    "\n",
    "    def predict(self, df, features):\n",
    "        X = df[features].values\n",
    "        dmatrix = xgb.DMatrix(X)\n",
    "        preds = np.mean([m.predict(dmatrix) for m in self.models], axis=0)\n",
    "        return preds\n",
    "\n",
    "    def evaluate(self, df, features, risk_free_col=None):\n",
    "        \"\"\"\n",
    "        Evaluate model performance using both regression and trading strategy metrics.\n",
    "\n",
    "        Key fix: Use actual returns (y_true) for strategy evaluation, not predictions.\n",
    "        Predictions are used to decide position sizing/direction.\n",
    "        \"\"\"\n",
    "        y_true = df[self.target_col].values\n",
    "        y_pred = self.predict(df, features)\n",
    "\n",
    "        # Regression Metrics\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = root_mean_squared_error(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "        # Trading Strategy Metrics\n",
    "        # Strategy: Take position proportional to predicted excess returns\n",
    "        # Actual returns are the realized returns based on our positions\n",
    "\n",
    "        # Simple strategy: if prediction > 0, go long; if < 0, go short\n",
    "        positions = y_pred  # Try to instead use signals 0 negative , 1 for positie\n",
    "\n",
    "        # Actual strategy returns = position * actual returns\n",
    "        strategy_returns = positions * y_true\n",
    "\n",
    "        # Calculate Sharpe ratio on strategy returns\n",
    "        mean_return = np.mean(strategy_returns)\n",
    "        std_return = np.std(strategy_returns)\n",
    "        sharpe_ratio = (mean_return / (std_return + 1e-8)) * np.sqrt(252)\n",
    "\n",
    "        return {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'Sharpe Ratio': sharpe_ratio,\n",
    "        }\n",
    "    def market_allocation(self, preds):\n",
    "      \"\"\"\n",
    "      Based on the predicted returns, give an allocation value between [0-2], k = sensitivity factor\n",
    "      \"\"\"\n",
    "      market_returns = preds\n",
    "      k = 75 # sensitivity factor\n",
    "      allocations = 2 / (1 + np.exp(-k * market_returns))\n",
    "      return allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6424ee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.00731\tval-rmse:0.01209\n",
      "[1]\ttrain-rmse:0.00730\tval-rmse:0.01208\n",
      "[2]\ttrain-rmse:0.00728\tval-rmse:0.01209\n",
      "[3]\ttrain-rmse:0.00727\tval-rmse:0.01209\n",
      "[4]\ttrain-rmse:0.00726\tval-rmse:0.01212\n",
      "[5]\ttrain-rmse:0.00724\tval-rmse:0.01212\n",
      "[6]\ttrain-rmse:0.00723\tval-rmse:0.01212\n",
      "[7]\ttrain-rmse:0.00721\tval-rmse:0.01211\n",
      "[8]\ttrain-rmse:0.00720\tval-rmse:0.01212\n",
      "[9]\ttrain-rmse:0.00718\tval-rmse:0.01213\n",
      "[10]\ttrain-rmse:0.00717\tval-rmse:0.01214\n",
      "[11]\ttrain-rmse:0.00715\tval-rmse:0.01214\n",
      "[12]\ttrain-rmse:0.00714\tval-rmse:0.01215\n",
      "[13]\ttrain-rmse:0.00713\tval-rmse:0.01214\n",
      "[14]\ttrain-rmse:0.00712\tval-rmse:0.01214\n",
      "[15]\ttrain-rmse:0.00711\tval-rmse:0.01214\n",
      "[16]\ttrain-rmse:0.00710\tval-rmse:0.01213\n",
      "[17]\ttrain-rmse:0.00709\tval-rmse:0.01213\n",
      "[18]\ttrain-rmse:0.00708\tval-rmse:0.01213\n",
      "[19]\ttrain-rmse:0.00706\tval-rmse:0.01213\n",
      "[20]\ttrain-rmse:0.00705\tval-rmse:0.01214\n",
      "[21]\ttrain-rmse:0.00705\tval-rmse:0.01213\n",
      "[22]\ttrain-rmse:0.00703\tval-rmse:0.01213\n",
      "[23]\ttrain-rmse:0.00703\tval-rmse:0.01214\n",
      "[24]\ttrain-rmse:0.00702\tval-rmse:0.01213\n",
      "[25]\ttrain-rmse:0.00701\tval-rmse:0.01214\n",
      "[26]\ttrain-rmse:0.00700\tval-rmse:0.01217\n",
      "[27]\ttrain-rmse:0.00700\tval-rmse:0.01217\n",
      "[28]\ttrain-rmse:0.00699\tval-rmse:0.01217\n",
      "[29]\ttrain-rmse:0.00698\tval-rmse:0.01217\n",
      "[30]\ttrain-rmse:0.00697\tval-rmse:0.01217\n",
      "[31]\ttrain-rmse:0.00696\tval-rmse:0.01216\n",
      "[32]\ttrain-rmse:0.00695\tval-rmse:0.01214\n",
      "[33]\ttrain-rmse:0.00694\tval-rmse:0.01215\n",
      "[34]\ttrain-rmse:0.00693\tval-rmse:0.01215\n",
      "[35]\ttrain-rmse:0.00692\tval-rmse:0.01216\n",
      "[36]\ttrain-rmse:0.00691\tval-rmse:0.01216\n",
      "[37]\ttrain-rmse:0.00690\tval-rmse:0.01216\n",
      "[38]\ttrain-rmse:0.00690\tval-rmse:0.01216\n",
      "[39]\ttrain-rmse:0.00689\tval-rmse:0.01216\n",
      "[40]\ttrain-rmse:0.00689\tval-rmse:0.01216\n",
      "[41]\ttrain-rmse:0.00688\tval-rmse:0.01216\n",
      "[42]\ttrain-rmse:0.00688\tval-rmse:0.01216\n",
      "[43]\ttrain-rmse:0.00687\tval-rmse:0.01217\n",
      "[44]\ttrain-rmse:0.00686\tval-rmse:0.01215\n",
      "[45]\ttrain-rmse:0.00686\tval-rmse:0.01216\n",
      "[46]\ttrain-rmse:0.00686\tval-rmse:0.01216\n",
      "[47]\ttrain-rmse:0.00685\tval-rmse:0.01216\n",
      "[48]\ttrain-rmse:0.00684\tval-rmse:0.01216\n",
      "[49]\ttrain-rmse:0.00684\tval-rmse:0.01218\n",
      "[50]\ttrain-rmse:0.00683\tval-rmse:0.01218\n",
      "[51]\ttrain-rmse:0.00682\tval-rmse:0.01218\n",
      "Fold 1 finished. Best Iteration: 1\n",
      "[0]\ttrain-rmse:0.00996\tval-rmse:0.00981\n",
      "[1]\ttrain-rmse:0.00992\tval-rmse:0.00980\n",
      "[2]\ttrain-rmse:0.00987\tval-rmse:0.00979\n",
      "[3]\ttrain-rmse:0.00981\tval-rmse:0.00980\n",
      "[4]\ttrain-rmse:0.00977\tval-rmse:0.00980\n",
      "[5]\ttrain-rmse:0.00972\tval-rmse:0.00980\n",
      "[6]\ttrain-rmse:0.00969\tval-rmse:0.00980\n",
      "[7]\ttrain-rmse:0.00965\tval-rmse:0.00980\n",
      "[8]\ttrain-rmse:0.00963\tval-rmse:0.00980\n",
      "[9]\ttrain-rmse:0.00959\tval-rmse:0.00980\n",
      "[10]\ttrain-rmse:0.00956\tval-rmse:0.00980\n",
      "[11]\ttrain-rmse:0.00952\tval-rmse:0.00982\n",
      "[12]\ttrain-rmse:0.00949\tval-rmse:0.00982\n",
      "[13]\ttrain-rmse:0.00945\tval-rmse:0.00984\n",
      "[14]\ttrain-rmse:0.00944\tval-rmse:0.00988\n",
      "[15]\ttrain-rmse:0.00941\tval-rmse:0.00988\n",
      "[16]\ttrain-rmse:0.00937\tval-rmse:0.00990\n",
      "[17]\ttrain-rmse:0.00935\tval-rmse:0.00991\n",
      "[18]\ttrain-rmse:0.00933\tval-rmse:0.00991\n",
      "[19]\ttrain-rmse:0.00930\tval-rmse:0.00991\n",
      "[20]\ttrain-rmse:0.00927\tval-rmse:0.00991\n",
      "[21]\ttrain-rmse:0.00925\tval-rmse:0.00993\n",
      "[22]\ttrain-rmse:0.00922\tval-rmse:0.00993\n",
      "[23]\ttrain-rmse:0.00918\tval-rmse:0.00993\n",
      "[24]\ttrain-rmse:0.00916\tval-rmse:0.00994\n",
      "[25]\ttrain-rmse:0.00912\tval-rmse:0.00994\n",
      "[26]\ttrain-rmse:0.00910\tval-rmse:0.00995\n",
      "[27]\ttrain-rmse:0.00907\tval-rmse:0.00997\n",
      "[28]\ttrain-rmse:0.00905\tval-rmse:0.00997\n",
      "[29]\ttrain-rmse:0.00902\tval-rmse:0.00998\n",
      "[30]\ttrain-rmse:0.00900\tval-rmse:0.00998\n",
      "[31]\ttrain-rmse:0.00897\tval-rmse:0.00998\n",
      "[32]\ttrain-rmse:0.00895\tval-rmse:0.00998\n",
      "[33]\ttrain-rmse:0.00892\tval-rmse:0.01007\n",
      "[34]\ttrain-rmse:0.00890\tval-rmse:0.01008\n",
      "[35]\ttrain-rmse:0.00888\tval-rmse:0.01008\n",
      "[36]\ttrain-rmse:0.00886\tval-rmse:0.01008\n",
      "[37]\ttrain-rmse:0.00883\tval-rmse:0.01008\n",
      "[38]\ttrain-rmse:0.00881\tval-rmse:0.01008\n",
      "[39]\ttrain-rmse:0.00879\tval-rmse:0.01009\n",
      "[40]\ttrain-rmse:0.00876\tval-rmse:0.01010\n",
      "[41]\ttrain-rmse:0.00873\tval-rmse:0.01010\n",
      "[42]\ttrain-rmse:0.00871\tval-rmse:0.01010\n",
      "[43]\ttrain-rmse:0.00869\tval-rmse:0.01011\n",
      "[44]\ttrain-rmse:0.00866\tval-rmse:0.01011\n",
      "[45]\ttrain-rmse:0.00865\tval-rmse:0.01014\n",
      "[46]\ttrain-rmse:0.00863\tval-rmse:0.01014\n",
      "[47]\ttrain-rmse:0.00861\tval-rmse:0.01015\n",
      "[48]\ttrain-rmse:0.00858\tval-rmse:0.01015\n",
      "[49]\ttrain-rmse:0.00856\tval-rmse:0.01017\n",
      "[50]\ttrain-rmse:0.00853\tval-rmse:0.01017\n",
      "[51]\ttrain-rmse:0.00852\tval-rmse:0.01018\n",
      "[52]\ttrain-rmse:0.00850\tval-rmse:0.01018\n",
      "Fold 2 finished. Best Iteration: 2\n",
      "[0]\ttrain-rmse:0.00990\tval-rmse:0.01330\n",
      "[1]\ttrain-rmse:0.00988\tval-rmse:0.01330\n",
      "[2]\ttrain-rmse:0.00984\tval-rmse:0.01329\n",
      "[3]\ttrain-rmse:0.00981\tval-rmse:0.01330\n",
      "[4]\ttrain-rmse:0.00978\tval-rmse:0.01331\n",
      "[5]\ttrain-rmse:0.00975\tval-rmse:0.01330\n",
      "[6]\ttrain-rmse:0.00971\tval-rmse:0.01329\n",
      "[7]\ttrain-rmse:0.00968\tval-rmse:0.01331\n",
      "[8]\ttrain-rmse:0.00966\tval-rmse:0.01331\n",
      "[9]\ttrain-rmse:0.00963\tval-rmse:0.01330\n",
      "[10]\ttrain-rmse:0.00960\tval-rmse:0.01329\n",
      "[11]\ttrain-rmse:0.00957\tval-rmse:0.01330\n",
      "[12]\ttrain-rmse:0.00956\tval-rmse:0.01329\n",
      "[13]\ttrain-rmse:0.00953\tval-rmse:0.01330\n",
      "[14]\ttrain-rmse:0.00951\tval-rmse:0.01330\n",
      "[15]\ttrain-rmse:0.00949\tval-rmse:0.01329\n",
      "[16]\ttrain-rmse:0.00946\tval-rmse:0.01329\n",
      "[17]\ttrain-rmse:0.00944\tval-rmse:0.01329\n",
      "[18]\ttrain-rmse:0.00942\tval-rmse:0.01329\n",
      "[19]\ttrain-rmse:0.00940\tval-rmse:0.01330\n",
      "[20]\ttrain-rmse:0.00938\tval-rmse:0.01331\n",
      "[21]\ttrain-rmse:0.00935\tval-rmse:0.01334\n",
      "[22]\ttrain-rmse:0.00931\tval-rmse:0.01335\n",
      "[23]\ttrain-rmse:0.00930\tval-rmse:0.01337\n",
      "[24]\ttrain-rmse:0.00928\tval-rmse:0.01339\n",
      "[25]\ttrain-rmse:0.00926\tval-rmse:0.01339\n",
      "[26]\ttrain-rmse:0.00923\tval-rmse:0.01340\n",
      "[27]\ttrain-rmse:0.00921\tval-rmse:0.01339\n",
      "[28]\ttrain-rmse:0.00918\tval-rmse:0.01340\n",
      "[29]\ttrain-rmse:0.00915\tval-rmse:0.01340\n",
      "[30]\ttrain-rmse:0.00914\tval-rmse:0.01340\n",
      "[31]\ttrain-rmse:0.00912\tval-rmse:0.01340\n",
      "[32]\ttrain-rmse:0.00910\tval-rmse:0.01340\n",
      "[33]\ttrain-rmse:0.00908\tval-rmse:0.01339\n",
      "[34]\ttrain-rmse:0.00906\tval-rmse:0.01339\n",
      "[35]\ttrain-rmse:0.00903\tval-rmse:0.01339\n",
      "[36]\ttrain-rmse:0.00902\tval-rmse:0.01339\n",
      "[37]\ttrain-rmse:0.00899\tval-rmse:0.01339\n",
      "[38]\ttrain-rmse:0.00897\tval-rmse:0.01339\n",
      "[39]\ttrain-rmse:0.00896\tval-rmse:0.01339\n",
      "[40]\ttrain-rmse:0.00893\tval-rmse:0.01339\n",
      "[41]\ttrain-rmse:0.00892\tval-rmse:0.01341\n",
      "[42]\ttrain-rmse:0.00890\tval-rmse:0.01341\n",
      "[43]\ttrain-rmse:0.00889\tval-rmse:0.01340\n",
      "[44]\ttrain-rmse:0.00887\tval-rmse:0.01340\n",
      "[45]\ttrain-rmse:0.00885\tval-rmse:0.01340\n",
      "[46]\ttrain-rmse:0.00883\tval-rmse:0.01341\n",
      "[47]\ttrain-rmse:0.00881\tval-rmse:0.01344\n",
      "[48]\ttrain-rmse:0.00880\tval-rmse:0.01345\n",
      "[49]\ttrain-rmse:0.00878\tval-rmse:0.01345\n",
      "[50]\ttrain-rmse:0.00875\tval-rmse:0.01346\n",
      "[51]\ttrain-rmse:0.00873\tval-rmse:0.01346\n",
      "[52]\ttrain-rmse:0.00870\tval-rmse:0.01347\n",
      "[53]\ttrain-rmse:0.00869\tval-rmse:0.01347\n",
      "[54]\ttrain-rmse:0.00867\tval-rmse:0.01348\n",
      "[55]\ttrain-rmse:0.00865\tval-rmse:0.01347\n",
      "[56]\ttrain-rmse:0.00864\tval-rmse:0.01347\n",
      "[57]\ttrain-rmse:0.00863\tval-rmse:0.01350\n",
      "[58]\ttrain-rmse:0.00861\tval-rmse:0.01349\n",
      "[59]\ttrain-rmse:0.00860\tval-rmse:0.01350\n",
      "[60]\ttrain-rmse:0.00858\tval-rmse:0.01349\n",
      "[61]\ttrain-rmse:0.00857\tval-rmse:0.01350\n",
      "[62]\ttrain-rmse:0.00855\tval-rmse:0.01350\n",
      "[63]\ttrain-rmse:0.00854\tval-rmse:0.01351\n",
      "[64]\ttrain-rmse:0.00852\tval-rmse:0.01351\n",
      "[65]\ttrain-rmse:0.00851\tval-rmse:0.01352\n",
      "[66]\ttrain-rmse:0.00850\tval-rmse:0.01351\n",
      "[67]\ttrain-rmse:0.00848\tval-rmse:0.01351\n",
      "Fold 3 finished. Best Iteration: 17\n",
      "[0]\ttrain-rmse:0.01084\tval-rmse:0.00819\n",
      "[1]\ttrain-rmse:0.01079\tval-rmse:0.00819\n",
      "[2]\ttrain-rmse:0.01075\tval-rmse:0.00820\n",
      "[3]\ttrain-rmse:0.01071\tval-rmse:0.00820\n",
      "[4]\ttrain-rmse:0.01068\tval-rmse:0.00819\n",
      "[5]\ttrain-rmse:0.01065\tval-rmse:0.00819\n",
      "[6]\ttrain-rmse:0.01061\tval-rmse:0.00819\n",
      "[7]\ttrain-rmse:0.01058\tval-rmse:0.00818\n",
      "[8]\ttrain-rmse:0.01056\tval-rmse:0.00818\n",
      "[9]\ttrain-rmse:0.01052\tval-rmse:0.00817\n",
      "[10]\ttrain-rmse:0.01048\tval-rmse:0.00816\n",
      "[11]\ttrain-rmse:0.01045\tval-rmse:0.00817\n",
      "[12]\ttrain-rmse:0.01042\tval-rmse:0.00817\n",
      "[13]\ttrain-rmse:0.01039\tval-rmse:0.00817\n",
      "[14]\ttrain-rmse:0.01037\tval-rmse:0.00816\n",
      "[15]\ttrain-rmse:0.01034\tval-rmse:0.00817\n",
      "[16]\ttrain-rmse:0.01032\tval-rmse:0.00818\n",
      "[17]\ttrain-rmse:0.01030\tval-rmse:0.00817\n",
      "[18]\ttrain-rmse:0.01027\tval-rmse:0.00818\n",
      "[19]\ttrain-rmse:0.01025\tval-rmse:0.00818\n",
      "[20]\ttrain-rmse:0.01023\tval-rmse:0.00819\n",
      "[21]\ttrain-rmse:0.01020\tval-rmse:0.00819\n",
      "[22]\ttrain-rmse:0.01018\tval-rmse:0.00820\n",
      "[23]\ttrain-rmse:0.01017\tval-rmse:0.00820\n",
      "[24]\ttrain-rmse:0.01015\tval-rmse:0.00820\n",
      "[25]\ttrain-rmse:0.01013\tval-rmse:0.00821\n",
      "[26]\ttrain-rmse:0.01010\tval-rmse:0.00820\n",
      "[27]\ttrain-rmse:0.01009\tval-rmse:0.00821\n",
      "[28]\ttrain-rmse:0.01006\tval-rmse:0.00820\n",
      "[29]\ttrain-rmse:0.01005\tval-rmse:0.00820\n",
      "[30]\ttrain-rmse:0.01003\tval-rmse:0.00822\n",
      "[31]\ttrain-rmse:0.01001\tval-rmse:0.00823\n",
      "[32]\ttrain-rmse:0.00999\tval-rmse:0.00824\n",
      "[33]\ttrain-rmse:0.00997\tval-rmse:0.00824\n",
      "[34]\ttrain-rmse:0.00996\tval-rmse:0.00823\n",
      "[35]\ttrain-rmse:0.00994\tval-rmse:0.00824\n",
      "[36]\ttrain-rmse:0.00992\tval-rmse:0.00823\n",
      "[37]\ttrain-rmse:0.00990\tval-rmse:0.00823\n",
      "[38]\ttrain-rmse:0.00988\tval-rmse:0.00824\n",
      "[39]\ttrain-rmse:0.00986\tval-rmse:0.00825\n",
      "[40]\ttrain-rmse:0.00985\tval-rmse:0.00825\n",
      "[41]\ttrain-rmse:0.00983\tval-rmse:0.00825\n",
      "[42]\ttrain-rmse:0.00981\tval-rmse:0.00825\n",
      "[43]\ttrain-rmse:0.00979\tval-rmse:0.00825\n",
      "[44]\ttrain-rmse:0.00977\tval-rmse:0.00826\n",
      "[45]\ttrain-rmse:0.00975\tval-rmse:0.00827\n",
      "[46]\ttrain-rmse:0.00974\tval-rmse:0.00827\n",
      "[47]\ttrain-rmse:0.00972\tval-rmse:0.00827\n",
      "[48]\ttrain-rmse:0.00971\tval-rmse:0.00827\n",
      "[49]\ttrain-rmse:0.00969\tval-rmse:0.00829\n",
      "[50]\ttrain-rmse:0.00966\tval-rmse:0.00830\n",
      "[51]\ttrain-rmse:0.00965\tval-rmse:0.00830\n",
      "[52]\ttrain-rmse:0.00963\tval-rmse:0.00830\n",
      "[53]\ttrain-rmse:0.00961\tval-rmse:0.00831\n",
      "[54]\ttrain-rmse:0.00959\tval-rmse:0.00833\n",
      "[55]\ttrain-rmse:0.00958\tval-rmse:0.00833\n",
      "[56]\ttrain-rmse:0.00956\tval-rmse:0.00834\n",
      "[57]\ttrain-rmse:0.00954\tval-rmse:0.00834\n",
      "[58]\ttrain-rmse:0.00953\tval-rmse:0.00833\n",
      "[59]\ttrain-rmse:0.00951\tval-rmse:0.00833\n",
      "[60]\ttrain-rmse:0.00950\tval-rmse:0.00834\n",
      "Fold 4 finished. Best Iteration: 10\n",
      "[0]\ttrain-rmse:0.01036\tval-rmse:0.01140\n",
      "[1]\ttrain-rmse:0.01033\tval-rmse:0.01139\n",
      "[2]\ttrain-rmse:0.01031\tval-rmse:0.01139\n",
      "[3]\ttrain-rmse:0.01028\tval-rmse:0.01140\n",
      "[4]\ttrain-rmse:0.01026\tval-rmse:0.01139\n",
      "[5]\ttrain-rmse:0.01023\tval-rmse:0.01139\n",
      "[6]\ttrain-rmse:0.01020\tval-rmse:0.01139\n",
      "[7]\ttrain-rmse:0.01017\tval-rmse:0.01138\n",
      "[8]\ttrain-rmse:0.01016\tval-rmse:0.01138\n",
      "[9]\ttrain-rmse:0.01012\tval-rmse:0.01139\n",
      "[10]\ttrain-rmse:0.01009\tval-rmse:0.01138\n",
      "[11]\ttrain-rmse:0.01006\tval-rmse:0.01139\n",
      "[12]\ttrain-rmse:0.01004\tval-rmse:0.01140\n",
      "[13]\ttrain-rmse:0.01002\tval-rmse:0.01140\n",
      "[14]\ttrain-rmse:0.01001\tval-rmse:0.01140\n",
      "[15]\ttrain-rmse:0.00998\tval-rmse:0.01141\n",
      "[16]\ttrain-rmse:0.00996\tval-rmse:0.01141\n",
      "[17]\ttrain-rmse:0.00994\tval-rmse:0.01141\n",
      "[18]\ttrain-rmse:0.00991\tval-rmse:0.01141\n",
      "[19]\ttrain-rmse:0.00990\tval-rmse:0.01142\n",
      "[20]\ttrain-rmse:0.00988\tval-rmse:0.01143\n",
      "[21]\ttrain-rmse:0.00986\tval-rmse:0.01144\n",
      "[22]\ttrain-rmse:0.00984\tval-rmse:0.01145\n",
      "[23]\ttrain-rmse:0.00983\tval-rmse:0.01145\n",
      "[24]\ttrain-rmse:0.00980\tval-rmse:0.01145\n",
      "[25]\ttrain-rmse:0.00978\tval-rmse:0.01145\n",
      "[26]\ttrain-rmse:0.00976\tval-rmse:0.01145\n",
      "[27]\ttrain-rmse:0.00975\tval-rmse:0.01146\n",
      "[28]\ttrain-rmse:0.00973\tval-rmse:0.01146\n",
      "[29]\ttrain-rmse:0.00972\tval-rmse:0.01146\n",
      "[30]\ttrain-rmse:0.00970\tval-rmse:0.01146\n",
      "[31]\ttrain-rmse:0.00968\tval-rmse:0.01147\n",
      "[32]\ttrain-rmse:0.00966\tval-rmse:0.01148\n",
      "[33]\ttrain-rmse:0.00963\tval-rmse:0.01148\n",
      "[34]\ttrain-rmse:0.00962\tval-rmse:0.01148\n",
      "[35]\ttrain-rmse:0.00960\tval-rmse:0.01147\n",
      "[36]\ttrain-rmse:0.00958\tval-rmse:0.01149\n",
      "[37]\ttrain-rmse:0.00956\tval-rmse:0.01149\n",
      "[38]\ttrain-rmse:0.00953\tval-rmse:0.01150\n",
      "[39]\ttrain-rmse:0.00952\tval-rmse:0.01149\n",
      "[40]\ttrain-rmse:0.00950\tval-rmse:0.01150\n",
      "[41]\ttrain-rmse:0.00948\tval-rmse:0.01149\n",
      "[42]\ttrain-rmse:0.00947\tval-rmse:0.01148\n",
      "[43]\ttrain-rmse:0.00945\tval-rmse:0.01149\n",
      "[44]\ttrain-rmse:0.00943\tval-rmse:0.01148\n",
      "[45]\ttrain-rmse:0.00942\tval-rmse:0.01149\n",
      "[46]\ttrain-rmse:0.00940\tval-rmse:0.01149\n",
      "[47]\ttrain-rmse:0.00939\tval-rmse:0.01149\n",
      "[48]\ttrain-rmse:0.00938\tval-rmse:0.01151\n",
      "[49]\ttrain-rmse:0.00936\tval-rmse:0.01150\n",
      "[50]\ttrain-rmse:0.00934\tval-rmse:0.01150\n",
      "[51]\ttrain-rmse:0.00933\tval-rmse:0.01150\n",
      "[52]\ttrain-rmse:0.00931\tval-rmse:0.01150\n",
      "[53]\ttrain-rmse:0.00930\tval-rmse:0.01150\n",
      "[54]\ttrain-rmse:0.00929\tval-rmse:0.01150\n",
      "[55]\ttrain-rmse:0.00927\tval-rmse:0.01149\n",
      "[56]\ttrain-rmse:0.00926\tval-rmse:0.01150\n",
      "[57]\ttrain-rmse:0.00925\tval-rmse:0.01150\n",
      "[58]\ttrain-rmse:0.00924\tval-rmse:0.01150\n",
      "Fold 5 finished. Best Iteration: 8\n",
      "\n",
      "=== Train Evaluation Results ===\n",
      "MSE: 0.000099\n",
      "RMSE: 0.009956\n",
      "Sharpe Ratio: 2.907751\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = ['date_id', 'forward_returns', 'risk_free_rate',\n",
    "                'market_forward_excess_returns', 'is_scored',\n",
    "                'lagged_forward_returns', 'lagged_risk_free_rate',\n",
    "                'lagged_market_forward_excess_returns']\n",
    "\n",
    "# ---------------------- Initialize Feature Engineering ----------------------\n",
    "ts_engineer = TimeSeriesEngineering(\n",
    "    base_features=['M1','V1','S1','P1','E1'], # base features to prevent exploding the dataset\n",
    "    lag_windows=[1,5],\n",
    "    rolling_windows=[5,10,20],\n",
    "    ema_windows=[10,30,60],\n",
    "    sparse_cols=['M6','M13','M14','V9','V10','S3','E7','D2'],\n",
    "    scale_features=True\n",
    ")\n",
    "\n",
    "# ---------------------- Preprocess Training Set ----------------------\n",
    "train_df_fe = ts_engineer.transform(train_df, fit=True)\n",
    "features = [c for c in train_df_fe.columns if c not in exclude_cols]\n",
    "\n",
    "# ---------------------- Train Time Series Model ----------------------\n",
    "xgb_tcsv = TimeSeriesModel(\n",
    "    target_col='market_forward_excess_returns',\n",
    "    n_splits=5\n",
    ")\n",
    "xgb_tcsv.fit(train_df_fe, features)\n",
    "\n",
    "# ---------------------- Evaluate on Training Set ----------------------\n",
    "train_eval_results = xgb_tcsv.evaluate(train_df_fe, features)\n",
    "print(\"\\n=== Train Evaluation Results ===\")\n",
    "for metric, value in train_eval_results.items():\n",
    "    if metric != 'Cumulative Returns':\n",
    "        print(f\"{metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f628f8f3",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The sharpe ratio [1-3], which is good for real-markets.\n",
    "- Might need to introduce scale, or signals because values for market_forward_excess_rates are very small or in percentages.\n",
    "- Need to combine triaining and test since the sample size is so small, the evaluation on the test set is insignificant\n",
    "- Probably need also try different lag values, since"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hull_Tactical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
