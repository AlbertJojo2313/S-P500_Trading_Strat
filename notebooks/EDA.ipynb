{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b1eKZaZbosZh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import timedelta, datetime\n",
        "from scipy import stats\n",
        "from scipy.stats import shapiro\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-F_GLEjrA7g"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJAVyuxQr74Y"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVtKYYFgsABN"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDQAYtxnsN8o"
      },
      "outputs": [],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPaHcLOysQvh"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e8Tg6_28fvv"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akOJ9YbWsbvs"
      },
      "outputs": [],
      "source": [
        "##Find missing values for each column\n",
        "train_df.isnull().sum().groupby(train_df.columns).agg(['sum']).sort_values(by='sum', ascending=False)\n",
        "## Find the missing percent\n",
        "train_df.isnull().mean().groupby(train_df.columns).agg(['mean']).sort_values(by='mean', ascending=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZKy2VgItdD4"
      },
      "source": [
        "## Response Variable Analysis\n",
        "- forward_returns\n",
        "- market_forward_excess_returns\n",
        "- risk_free_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVD3RM2Ctcd3"
      },
      "outputs": [],
      "source": [
        "response_col = ['forward_returns', 'market_forward_excess_returns', 'risk_free_rate']\n",
        "response_col = [col for col in train_df.columns if col in response_col]\n",
        "\n",
        "#Plot the distributions\n",
        "fig, axs = plt.subplots(4, len(response_col),figsize=(30,10))\n",
        "for i, col in enumerate(response_col):\n",
        "  data = train_df[col].dropna()\n",
        "\n",
        "  print(f'{col} Stats')\n",
        "  print(f'Skewness: {data.skew():3f}')\n",
        "  print(f'Kurtosis: {data.kurtosis():3f}')\n",
        "\n",
        "  sns.histplot(data=data, ax=axs[0,i])\n",
        "  axs[0,i].axvline(data.mean(), linestyle='--', color='red', label=f'{col} Mean:{data.mean():.4f}')\n",
        "  axs[0,i].axvline(data.median(), linestyle='--', color='blue', label=f'{col} Median:{data.median():.4f}')\n",
        "  axs[0,i].set_title(f'{col} Distribution')\n",
        "  axs[0,i].legend()\n",
        "\n",
        "  sns.histplot(data=data,stat='density',ax=axs[1,i])\n",
        "  sns.kdeplot(data=data, color='red', lw=2, ax=axs[1, i])\n",
        "  axs[1,i].set_title(f'{col} Histogram with Density Plot')\n",
        "\n",
        "  sns.lineplot(data=data, ax=axs[2,i])\n",
        "  axs[2,i].set_title(f'{col} Time Series Plot')\n",
        "\n",
        "  #Q-Q Plot\n",
        "  stats.probplot(data, dist='norm', plot=axs[3,i])\n",
        "\n",
        "\n",
        "\n",
        "  ## Shapire-Wilkins test\n",
        "  statistic, p = shapiro(data)\n",
        "  print(f\"Shapiro-Wilk Test: Statistic={round(statistic,3)}, p={round(statistic,3)}, is_normal={p>0.05}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('response_analysis.png')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e11b73d7"
      },
      "source": [
        "## Analysis\n",
        "Based on numeric and graphic representations of the distribution of hte response variable, we can identify that they're not normally distributed.\n",
        "\n",
        "\n",
        "**Note:** Can't use model that rely on normality assumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJYL5NaRVgH"
      },
      "source": [
        "## Feature Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEZ__Fp9QrtH"
      },
      "outputs": [],
      "source": [
        "feature_map = {\n",
        "    \"Dummy/Binary features(D)\": [col for col in train_df.columns if col.startswith('D') and col[1:].isnumeric()],\n",
        "    \"Macro Economic features(E)\": [col for col in train_df.columns if col.startswith('E') and col[1:].isnumeric()],\n",
        "    \"Interest Rate features(I)\": [col for col in train_df.columns if col.startswith('I') and col[1:].isnumeric()],\n",
        "    \"Market Dynamics/Technical features(M)\": [col for col in train_df.columns if col.startswith('M') and col[1:].isnumeric()],\n",
        "    \"Price/Valuation features(P)\": [col for col in train_df.columns if col.startswith('P') and col[1:].isnumeric()],\n",
        "    \"Sentiment features(S)\": [col for col in train_df.columns if col.startswith('S') and col[1:].isnumeric()],\n",
        "    \"Volatility features(V)\": [col for col in train_df.columns if col.startswith('V') and col[1:].isnumeric()]\n",
        "}\n",
        "for group_name, feature_group in feature_map.items():\n",
        "    if not feature_group:\n",
        "        continue\n",
        "\n",
        "    num_cols = 4\n",
        "    num_rows = int(np.ceil(len(feature_group) / num_cols))\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n",
        "    axs = axs.ravel() if len(feature_group) > 1 else [axs]\n",
        "    fig.suptitle(f\"{group_name}\", fontsize=16, y=1.02)\n",
        "\n",
        "    for idx, col in enumerate(feature_group):\n",
        "        data = train_df[col].dropna()\n",
        "\n",
        "        axs[idx].hist(x=data, bins=30, color='skyblue', edgecolor='black')\n",
        "        axs[idx].axvline(data.mean(), linestyle='--', color='red', label=f'Mean: {data.mean():.3f}')\n",
        "        axs[idx].axvline(data.median(), linestyle='--', color='blue', label='Median')\n",
        "        axs[idx].set_xlabel(col)\n",
        "        axs[idx].set_ylabel('Frequency')\n",
        "        axs[idx].set_title(f'{col} Distribution')\n",
        "        axs[idx].legend()\n",
        "\n",
        "\n",
        "    # Remove unused subplots\n",
        "    for j in range(len(feature_group), len(axs)):\n",
        "        fig.delaxes(axs[j])\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56687a06"
      },
      "source": [
        "### Observations\n",
        "\n",
        "Dummy Variables (D): are binary observations. Furthermore there is an imbalance of these observations, having more False's as Outcomes than True.\n",
        "\n",
        "* * *\n",
        "Economics (E):\n",
        "\n",
        "- Skewness\n",
        "  1. E1,E4, E6, E9, E11-E14, E19: Right Skewed, meaning most values are clustered around 0. Presence of extreme outliers.\n",
        "  2. E8,E16, E17: showcase light left skewness or bimodality\n",
        "- Sparse Features\n",
        "  1. E4,E6, E11-E14, have small ranges with majority of value clustering around 0.\n",
        "  - Could represent low variance.\n",
        "  - Might need scaling adjustments or removal if they are not significant\n",
        "- Uniform\n",
        "  1. E5, E10, & E15 looks to be a uniform distribution\n",
        "  - Might be useful in capturing non-linear distributions.\n",
        "- Mean vs. Median\n",
        "  1. Distributions showcase large gap between mean and median, reinforces high-skewness\n",
        "  - Mean > Median: positive skew\n",
        "  - Might require log-transformation or standardizing.\n",
        "- Potential Outliers\n",
        "  - Several variables showcases extreme spikes, or skewness, indicating extreme outliers\n",
        "  - Might need to use impute them using winzorized means.\n",
        "- Normal Distributions\n",
        "  - E3, E16, E17, E18, and E20 approximately normally distributed.\n",
        "- Modeling Implications:\n",
        "  - High-Skewness: Log-Scaling or some other transformations\n",
        "  - Biomodal: Clustering algos\n",
        "  - Normal: Linear Models\n",
        "  - Low-Variance: may be dropped or might need to combine them.\n",
        "\n",
        "* * *\n",
        "Interest(I)\n",
        "- Skewness:\n",
        "  1. I2: Right-skewed, potentially indicating the presence of outliers on the higher end.\n",
        "  2. I7, I2: Left-skewed, suggesting potential outliers on the lower end or a concentration of values towards the higher end.\n",
        "- Uniform Distribution:\n",
        "  - I3, I4, I6, I7, I8: These features appear to have a relatively uniform distribution, where values are spread somewhat evenly across the range.\n",
        "- Normal Distribution:\n",
        "  - I5, I9: These features seem to follow an approximately normal distribution.\n",
        "- Skewness and Mean vs. Median:\n",
        "  - The presence of skewness suggests the possibility of high or medium outliers, which could impact models sensitive to extreme values.\n",
        "  - The gap between the mean and median in some distributions reinforces the observation of skewness and potentially low variance in certain features.\n",
        "- Modeling Implications:\n",
        "  - Skewed distributions might require transformations (e.g., log transformation) to make them more symmetrical, which can improve the performance of some models.\n",
        "  - Features with low variance or uniform distributions might require careful consideration. They might not be highly informative on their own but could be useful in combination with other features or in certain model types.\n",
        "\n",
        "* * *\n",
        "Market(M)\n",
        "- Skewness:\n",
        "  1. M1, M10, M12, M13, M14, M15, M16, M17, M18, M2, M3, M4, M5, M6, M7, M8, M9: Many of the Market features exhibit varying degrees of skewness, both left and right. This indicates that the distribution of values is not symmetrical for these features.\n",
        "- Multimodality:\n",
        "  - Some features like M2, M3, and M6 show signs of multimodality (having multiple peaks in their distribution), suggesting that there might be distinct subgroups or regimes within the data that these features represent.\n",
        "- Potential Outliers:\n",
        "  - The presence of skewness and long tails in some distributions suggests the potential for outliers in the Market features. These outliers could disproportionately influence certain models.\n",
        "- Mean vs. Median:\n",
        "  - The difference between the mean and median in skewed distributions highlights the impact of outliers on the mean.\n",
        "- Modeling Implications:\n",
        "  - Skewed and multimodal distributions might require non-linear models or models that are robust to non-normal data.\n",
        "  - Multimodal features could indicate the need for clustering or segmentation of the data based on these features.\n",
        "  - Outliers might need to be handled through robust scaling methods, winsorization, or removal, depending on their nature and impact on the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVpG0Y7gmv6-"
      },
      "source": [
        "### Calculating Outliter Proportions for response and features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04NGz8rPorJS"
      },
      "source": [
        "#### Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzsfcmJXpNbR"
      },
      "outputs": [],
      "source": [
        "def outlier_prop(df, response_col):\n",
        "  outlier_props = {}\n",
        "\n",
        "  for response in response_col:\n",
        "    data = df[response].dropna()\n",
        "\n",
        "    ### Calculate Metrics\n",
        "    q1 = data.quantile(0.25)\n",
        "    q3 = data.quantile(0.75)\n",
        "\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "    outlier_prop = len(outliers) / len(data)\n",
        "    outlier_props[response] = outlier_prop\n",
        "\n",
        "  return outlier_props\n",
        "\n",
        "### Outlier props\n",
        "outlier_prop = outlier_prop(train_df, response_col)\n",
        "outlier_response_df = pd.DataFrame(list(outlier_prop.items()), columns=['Response', 'Outlier Proportion'])\n",
        "outlier_response_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIueYw6Dt5PC"
      },
      "outputs": [],
      "source": [
        "### Plot the props\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x='Response', y='Outlier Proportion', data=outlier_response_df, palette='viridis')\n",
        "plt.title('Outlier Proportion by Response Variable')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbJ51Es81DwK"
      },
      "outputs": [],
      "source": [
        "## Stationary\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "for col in response_col:\n",
        "  data = train_df[col].dropna()\n",
        "  result = adfuller(data)\n",
        "  print(f'{col} Stats')\n",
        "  print(f'ADF Statistic: {result[0]:.4f}')\n",
        "  print(f'p-value: {result[1]:.4f}, is_stationary: {result[1] < .05}\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jYj8GJavOQr"
      },
      "source": [
        "#### Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldNa2LnzmvPP"
      },
      "outputs": [],
      "source": [
        "def calc_outlier_proportion(series):\n",
        "    \"\"\"Calculate outlier proportion for a numeric column using IQR.\"\"\"\n",
        "    Q1 = series.quantile(0.25)\n",
        "    Q3 = series.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = ((series < lower) | (series > upper)).sum()\n",
        "    return outliers / series.notna().sum() if series.notna().sum() > 0 else np.nan\n",
        "\n",
        "outlier_summary = []\n",
        "\n",
        "for group_name, feature_group in feature_map.items():\n",
        "    for col in feature_group:\n",
        "        outlier_prop = calc_outlier_proportion(train_df[col])\n",
        "        outlier_summary.append({\n",
        "            'Group': group_name,\n",
        "            'Feature': col,\n",
        "            'Outlier Proportion': outlier_prop\n",
        "        })\n",
        "outlier_summary_df = pd.DataFrame(outlier_summary)\n",
        "\n",
        "## Group level summary\n",
        "group_avg = (\n",
        "    outlier_summary_df.groupby(\"Group\")[\"Outlier Proportion\"]\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "group_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-2sfIRGwqaZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "### Plot the proportion by group_avg\n",
        "pattern = r'\\((.*?)\\)'\n",
        "short_initials = [re.search(pattern, name).group(1) for name in group_avg.index]\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x=short_initials, y=group_avg.values, palette=\"viridis\")\n",
        "\n",
        "plt.title('Average Outlier Proportion by Feature Group')\n",
        "plt.xlabel('Feature Group (Short Code)')\n",
        "plt.ylabel('Average Outlier Proportion')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTjuDYRosSNc"
      },
      "source": [
        "#### Observation\n",
        "\n",
        "Forward_returns & Market_Forward_returns $\\approx 6.6. \\%$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MJyY_ksx8Hb"
      },
      "source": [
        "# Statistical Testing for Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvuCzvPJ0RbQ"
      },
      "source": [
        "### Stationary Test(ADF) Response\n",
        "$H_0$: non-stationary \\\n",
        "$H_A$: stationary\n",
        "\n",
        "If p< $\\alpha = 0.05$ (Stationary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR3wAVqX6wek"
      },
      "outputs": [],
      "source": [
        "for col in response_col:\n",
        "  data = train_df[col].dropna()\n",
        "  result = adfuller(data)\n",
        "\n",
        "  if result[1] >= .05:\n",
        "    print(f'{col} is non-stationary, p-value:{result[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlR2vnsD2GDC"
      },
      "source": [
        "### Observations:\n",
        "- forward_returns, market_forward_returns: Stationary\n",
        "- risk_free_rate : Non-Stationary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To_BLEhf2w6M"
      },
      "source": [
        "### Stationarity (Features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tpw0o5Rp2r4e"
      },
      "outputs": [],
      "source": [
        "non_stationary = {}\n",
        "\n",
        "\n",
        "for feature in train_df.columns[1:-1]:\n",
        "  col = train_df[feature].dropna()\n",
        "  result = adfuller(col)\n",
        "\n",
        "  if result[1] >= .05:\n",
        "    non_stationary[feature] = (result[0], result[1])\n",
        "    print(f'{feature} is non-stationary, p-value:{result[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65ABYO1t37du"
      },
      "outputs": [],
      "source": [
        "## Count of non-stationary features\n",
        "print(f'Number of non-stationary features: {len(non_stationary)}')\n",
        "\n",
        "\n",
        "## Calculate the proportion of non-stationary features\n",
        "for prefix in ['D','E','I','M','P','S']:\n",
        "  all_cols = [col for col in train_df.columns if col.startswith(prefix) and col[1:].isnumeric()]\n",
        "  non_stat = [col for col in all_cols if col in non_stationary]\n",
        "\n",
        "  print(f\"{prefix}, Count:{len(non_stat)}, Prop: {100 * len(non_stat) / len(all_cols) if len(all_cols) > 0 else 0:.3f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ifx36F8CJM"
      },
      "source": [
        "## Conclusion:\n",
        "Non-stationary features: ['E10', 'E11', 'E12', 'E2', 'E20', 'E3', 'E5', 'E6', 'E9', 'I4', 'I5', 'I8', 'I9', 'M13', 'M14', 'M16', 'M17', 'M18', 'P10', 'P11', 'P9', 'V11', 'V8', 'risk_free_rate']\n",
        "\n",
        "Need to adjusted or removed\n",
        "\n",
        "\n",
        "Stationary: Property or an assumption, that the statistical properties do not change over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XJmmvjx1tpg"
      },
      "source": [
        "## AutoCorrelation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfCdnXnS1xJj"
      },
      "source": [
        "#### Response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5S4y01c2o1Z"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "for col in ['market_forward_excess_returns']:\n",
        "  time_series_data = train_df[col].dropna()\n",
        "\n",
        "  # Plot ACF\n",
        "  fig, ax = plt.subplots(figsize=(10, 5))\n",
        "  plot_acf(time_series_data, ax=ax, lags=40)\n",
        "  ax.set_title(f'Autocorrelation Function (ACF) for {col}')\n",
        "  ax.set_xlabel('Lags')\n",
        "  ax.set_ylabel('Autocorrelation')\n",
        "  plt.savefig('autocorrelation.png')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot PACF\n",
        "  fig, ax = plt.subplots(figsize=(10,5))\n",
        "  plot_pacf(time_series_data, ax=ax, lags=40)\n",
        "  ax.set_title(f'Partial Autocorrelation Function (PACF) for {col}')\n",
        "  ax.set_xlabel('Lags')\n",
        "  ax.set_ylabel('Partial Autocorrelation')\n",
        "\n",
        "  plt.savefig('partial_autocorrelation.png')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOsXmAH3iez"
      },
      "source": [
        "### Observations\n",
        "PACF and ACF plots highlights how much a variable is related to its past values.\n",
        "\n",
        "- Forward_returns and market_forward excess_return, the plot suggests that current observation value is mainly related to values for the recent past, this relationship also fades very quickly thus highlighting that these variables are relatively stable over time\n",
        "- For risk_free rate: the plots suggest that past values have a longer-lasting impact, and the relationship decays more slowly; thus confirming that this reponse_variable is non-stationary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFAFMvcZ8YaA"
      },
      "source": [
        "### Normality (Features)\n",
        "\n",
        "$H_0$: Not normal \\\n",
        "$H_A$: Normal\n",
        "\n",
        "If p-val $ < \\alpha = .05$ (Normal)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObtmMW9f8YHA"
      },
      "outputs": [],
      "source": [
        "non_normal = {}\n",
        "for feature in train_df.columns[1:len(train_df) - 1]:\n",
        "  result = shapiro(train_df[feature].dropna())\n",
        "  if result[1] >= .05:\n",
        "    non_normal[feature] = (result[0], result[1])\n",
        "    print(f'{feature} is not normal, p-value:{result[1]:.3f}')\n",
        "  print(f\"{feature} is normal, p-value:{result[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiLrop9o97F9"
      },
      "source": [
        "### Conclusion\n",
        "Based on the p-value for all the features we can assume that all the distributions are normal, even the graph suggest otherwise on some variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mjMQT4z-aqF"
      },
      "source": [
        "### Homoscedasiticity (Constant Variance) (Response)\n",
        "\n",
        "$H_0$: Heteroscedasiticity \\\n",
        "$H_A$: Homoscedasiticity\n",
        "\n",
        "If p-val $ < \\alpha = .05$ (Homoscedasiticity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVMv3rft-h2J"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.diagnostic import het_arch\n",
        "\n",
        "for col in response_col:\n",
        "  data = train_df[col].dropna()\n",
        "  result = het_arch(data)\n",
        "  print(f'{col} Stats')\n",
        "  print(f'p-value: {result[1]:.4f}, is_homoscedastic: {result[1] < .05}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXeBaWch_mOd"
      },
      "outputs": [],
      "source": [
        "non_homoscedastic = {}\n",
        "for feature in train_df.columns[1:len(train_df) - 1]:\n",
        "  result = het_arch(train_df[feature].dropna())\n",
        "  if result[1] >= .05:\n",
        "    non_homoscedastic[feature] = (result[0], result[1])\n",
        "    print(f'{feature} is not homoscedastic, p-value:{result[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbVrP5B7_0nq"
      },
      "source": [
        "### Conclusion:\n",
        "\n",
        "From the previous conclusion, we concluded that most all the features were normal, thus, we inherently assume that they are homoscedastic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSX8Xf9AATl1"
      },
      "source": [
        "### Multicolinearity\n",
        "VIF < 8 (ideally < 3) means acceptable\n",
        "\n",
        "If high apply PCA or drop correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDCC3GvkAIW8"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "features = [col for col in train_df.columns if col not in [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"]]\n",
        "features_df = train_df[features].dropna()\n",
        "\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = features_df.columns\n",
        "vif_data[\"VIF\"] =[variance_inflation_factor(features_df.values, i)\n",
        "                    for i in\n",
        "range(len(features_df.columns))]\n",
        "\n",
        "vif_data.sort_values(by=\"VIF\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zw5hE_fjy9L"
      },
      "source": [
        "### Observation\n",
        "Based on the dataframe above we can either drop the high VIF columns greater than 8 or we can utilize PCA to reduce correlated features.\n",
        "\n",
        "### How to Interpret the Table\n",
        "The higher the VIF value, greater the feature correlation with another feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIsp3ACfChlr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "corr_matrix = features_df.corr()\n",
        "strong_corr_matrix = []\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i+1, len(corr_matrix.columns)):\n",
        "      if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
        "        strong_corr_matrix.append(\n",
        "            (corr_matrix.columns[i], corr_matrix.columns[j], abs(corr_matrix.iloc[i,j]))\n",
        "        )\n",
        "strong_corr_df = pd.DataFrame(strong_corr_matrix, columns=['Feature 1', 'Feature 2', 'Correlation'])\n",
        "print(strong_corr_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ljdVZcHmMf"
      },
      "outputs": [],
      "source": [
        "max_weak = abs(corr_matrix) < 0.5\n",
        "\n",
        "plt.figure(figsize=(30,15))\n",
        "sns.heatmap(\n",
        "    corr_matrix,\n",
        "    annot=False,\n",
        "    cmap='coolwarm',\n",
        "    mask=max_weak,\n",
        "    linewidths=0.5\n",
        ")\n",
        "plt.title('Strong Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeDq9Yjz7wmY"
      },
      "source": [
        "## Cross Correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mELC74It70SC"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import ccf\n",
        "\n",
        "feature_cols = [col for col in train_df.columns if col not in response_col]\n",
        "ccf_results = {}\n",
        "for feature in feature_cols:\n",
        "  for response in response_col:\n",
        "    temp_df = train_df[[feature, response]].dropna()\n",
        "    if len(temp_df) > 0:\n",
        "      cross_corr = ccf(temp_df[feature], temp_df[response], adjusted = False)\n",
        "\n",
        "      ccf_results[(feature, response)] = cross_corr\n",
        "      #Plotting\n",
        "      plt.figure(figsize=(10,5))\n",
        "      plt.stem(cross_corr)\n",
        "      plt.title(f'Cross-Correlation between {feature} and {response}')\n",
        "      plt.xlabel('Lag')\n",
        "      plt.ylabel('Cross-Correlation')\n",
        "      plt.grid(True)\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvzCpCCQ-Q4I"
      },
      "outputs": [],
      "source": [
        "ccf_summary = []\n",
        "for (feature, response), corr_values in ccf_results.items():\n",
        "    for lag, value in enumerate(corr_values):\n",
        "        ccf_summary.append({\n",
        "            'Feature': feature,\n",
        "            'Response': response,\n",
        "            'Lag': lag,\n",
        "            'CrossCorrelation': value\n",
        "        })\n",
        "ccf_df = pd.DataFrame(ccf_summary)\n",
        "ccf_df = ccf_df[ccf_df['Feature'] != 'date_id'] # Remove date_id\n",
        "\n",
        "ccf_df = ccf_df.loc[ccf_df['CrossCorrelation'].abs().sort_values(ascending=False).index]\n",
        "ccf_df.head(10) # Top 10 Cross correlated feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi4zo1CjFKw5"
      },
      "source": [
        "### Do T-test\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_KMgGMn9W8Nk"
      },
      "outputs": [],
      "source": [
        "ccf_df_group = ccf_df.groupby(['Feature','Response']).agg({\n",
        "    'Lag': 'mean',\n",
        "    'CrossCorrelation': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "ccf_df_group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eRz3uilWwxR"
      },
      "source": [
        "### Observation of Cross-Correlation\n",
        "\n",
        "- The dataframe above showcases the mean lag and mean cross-correlation for each feature-response pair.\n",
        "\n",
        "- The mean cross-correlation values accross all lags for most feature-response pairs are very close to 0.\n",
        "\n",
        "- This suggests that on average accross all lags, there isn't a strong linear relationship between the features and the response variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZfe4QXGZTCJ"
      },
      "source": [
        "### Insights\n",
        "**Response Variables**:\n",
        " - forward_returns and market_forward_excess_returns are stationary but not normally distributed and contain outliers; risk_free_rate is non-stationary.\n",
        "\n",
        "**Feature Properties**:\n",
        "\n",
        "- Many features are non-stationary (especially macroeconomic and interest rate groups) and show strong multicollinearity, particularly within binary, rate, and volatility groups.\n",
        "\n",
        "**Cross-Correlation**:\n",
        " - Most feature, response pairs have near zero mean correlation, but some show strong correlations at specific lags â€” features with negative lag correlation may act as leading indicators.\n",
        "\n",
        "**Missing Values**:\n",
        " - Present in several features need imputation or removal.\n",
        "\n",
        "### Modeling Recommendations\n",
        "\n",
        "**Time Series Baselines**:\n",
        "\n",
        " - ARIMA for stationary or differenced data.\n",
        "\n",
        " - Exponential Smoothing (Holt-Winters) for trend/seasonal components.\n",
        "\n",
        "**Feature-Based Models**:\n",
        "\n",
        "- Ridge / Lasso Regression to handle multicollinearity.\n",
        "\n",
        " - Tree-based models (Random Forest, XGBoost, LightGBM) for robustness to non-normality and nonlinear relationships.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Hull_Tactical",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
